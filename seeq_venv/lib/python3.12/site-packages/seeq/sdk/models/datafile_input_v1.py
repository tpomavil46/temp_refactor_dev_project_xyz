# coding: utf-8

"""
    Seeq REST API

    No description provided (generated by Swagger Codegen https://github.com/swagger-api/swagger-codegen)  # noqa: E501

    OpenAPI spec version: 66.10.0-v202502060217-CD
    
    Generated by: https://github.com/swagger-api/swagger-codegen.git
"""

from pprint import pformat
from six import iteritems
import re


class DatafileInputV1(object):
    """
    NOTE: This class is auto generated by the swagger code generator program.
    Do not edit the class manually.
    """


    """
    Attributes:
      swagger_types (dict): The key is attribute name
                            and the value is attribute type.
      attribute_map (dict): The key is attribute name
                            and the value is json key in definition.
    """
    swagger_types = {
        'additional_properties': 'list[ScalarPropertyV1]',
        'append': 'bool',
        'condition_name': 'str',
        'data_id': 'str',
        'day_first_default': 'bool',
        'description': 'str',
        'description_row': 'int',
        'end_column_index': 'int',
        'end_column_name': 'str',
        'field_delimiter': 'str',
        'filename': 'str',
        'first_data_row': 'int',
        'host_id': 'str',
        'interpolation_method': 'str',
        'interpolation_method_row': 'int',
        'item_type': 'str',
        'key_column_index': 'int',
        'key_column_name': 'str',
        'key_format': 'str',
        'lenient_daylight_savings': 'bool',
        'maximum_duration': 'str',
        'maximum_interpolation': 'str',
        'maximum_interpolation_row': 'int',
        'name': 'str',
        'name_prefix': 'str',
        'name_row': 'int',
        'name_suffix': 'str',
        'properties': 'list[ScalarPropertyV1]',
        'scoped_to': 'str',
        'time_zone': 'str',
        'upload_filename': 'str',
        'validation_mode': 'str',
        'value_column_indices': 'str',
        'value_column_names': 'str',
        'value_uom': 'str',
        'value_uom_row': 'int'
    }

    attribute_map = {
        'additional_properties': 'additionalProperties',
        'append': 'append',
        'condition_name': 'conditionName',
        'data_id': 'dataId',
        'day_first_default': 'dayFirstDefault',
        'description': 'description',
        'description_row': 'descriptionRow',
        'end_column_index': 'endColumnIndex',
        'end_column_name': 'endColumnName',
        'field_delimiter': 'fieldDelimiter',
        'filename': 'filename',
        'first_data_row': 'firstDataRow',
        'host_id': 'hostId',
        'interpolation_method': 'interpolationMethod',
        'interpolation_method_row': 'interpolationMethodRow',
        'item_type': 'itemType',
        'key_column_index': 'keyColumnIndex',
        'key_column_name': 'keyColumnName',
        'key_format': 'keyFormat',
        'lenient_daylight_savings': 'lenientDaylightSavings',
        'maximum_duration': 'maximumDuration',
        'maximum_interpolation': 'maximumInterpolation',
        'maximum_interpolation_row': 'maximumInterpolationRow',
        'name': 'name',
        'name_prefix': 'namePrefix',
        'name_row': 'nameRow',
        'name_suffix': 'nameSuffix',
        'properties': 'properties',
        'scoped_to': 'scopedTo',
        'time_zone': 'timeZone',
        'upload_filename': 'uploadFilename',
        'validation_mode': 'validationMode',
        'value_column_indices': 'valueColumnIndices',
        'value_column_names': 'valueColumnNames',
        'value_uom': 'valueUom',
        'value_uom_row': 'valueUomRow'
    }

    def __init__(self, additional_properties=None, append=False, condition_name=None, data_id=None, day_first_default=False, description=None, description_row=None, end_column_index=None, end_column_name=None, field_delimiter=None, filename=None, first_data_row=None, host_id=None, interpolation_method=None, interpolation_method_row=None, item_type=None, key_column_index=None, key_column_name=None, key_format=None, lenient_daylight_savings=False, maximum_duration=None, maximum_interpolation=None, maximum_interpolation_row=None, name=None, name_prefix=None, name_row=None, name_suffix=None, properties=None, scoped_to=None, time_zone=None, upload_filename=None, validation_mode=None, value_column_indices=None, value_column_names=None, value_uom=None, value_uom_row=None):
        """
        DatafileInputV1 - a model defined in Swagger
        """

        self._additional_properties = None
        self._append = None
        self._condition_name = None
        self._data_id = None
        self._day_first_default = None
        self._description = None
        self._description_row = None
        self._end_column_index = None
        self._end_column_name = None
        self._field_delimiter = None
        self._filename = None
        self._first_data_row = None
        self._host_id = None
        self._interpolation_method = None
        self._interpolation_method_row = None
        self._item_type = None
        self._key_column_index = None
        self._key_column_name = None
        self._key_format = None
        self._lenient_daylight_savings = None
        self._maximum_duration = None
        self._maximum_interpolation = None
        self._maximum_interpolation_row = None
        self._name = None
        self._name_prefix = None
        self._name_row = None
        self._name_suffix = None
        self._properties = None
        self._scoped_to = None
        self._time_zone = None
        self._upload_filename = None
        self._validation_mode = None
        self._value_column_indices = None
        self._value_column_names = None
        self._value_uom = None
        self._value_uom_row = None

        if additional_properties is not None:
          self.additional_properties = additional_properties
        if append is not None:
          self.append = append
        if condition_name is not None:
          self.condition_name = condition_name
        if data_id is not None:
          self.data_id = data_id
        if day_first_default is not None:
          self.day_first_default = day_first_default
        if description is not None:
          self.description = description
        if description_row is not None:
          self.description_row = description_row
        if end_column_index is not None:
          self.end_column_index = end_column_index
        if end_column_name is not None:
          self.end_column_name = end_column_name
        if field_delimiter is not None:
          self.field_delimiter = field_delimiter
        if filename is not None:
          self.filename = filename
        if first_data_row is not None:
          self.first_data_row = first_data_row
        if host_id is not None:
          self.host_id = host_id
        if interpolation_method is not None:
          self.interpolation_method = interpolation_method
        if interpolation_method_row is not None:
          self.interpolation_method_row = interpolation_method_row
        if item_type is not None:
          self.item_type = item_type
        if key_column_index is not None:
          self.key_column_index = key_column_index
        if key_column_name is not None:
          self.key_column_name = key_column_name
        if key_format is not None:
          self.key_format = key_format
        if lenient_daylight_savings is not None:
          self.lenient_daylight_savings = lenient_daylight_savings
        if maximum_duration is not None:
          self.maximum_duration = maximum_duration
        if maximum_interpolation is not None:
          self.maximum_interpolation = maximum_interpolation
        if maximum_interpolation_row is not None:
          self.maximum_interpolation_row = maximum_interpolation_row
        if name is not None:
          self.name = name
        if name_prefix is not None:
          self.name_prefix = name_prefix
        if name_row is not None:
          self.name_row = name_row
        if name_suffix is not None:
          self.name_suffix = name_suffix
        if properties is not None:
          self.properties = properties
        if scoped_to is not None:
          self.scoped_to = scoped_to
        if time_zone is not None:
          self.time_zone = time_zone
        if upload_filename is not None:
          self.upload_filename = upload_filename
        if validation_mode is not None:
          self.validation_mode = validation_mode
        if value_column_indices is not None:
          self.value_column_indices = value_column_indices
        if value_column_names is not None:
          self.value_column_names = value_column_names
        if value_uom is not None:
          self.value_uom = value_uom
        if value_uom_row is not None:
          self.value_uom_row = value_uom_row

    @property
    def additional_properties(self):
        """
        Gets the additional_properties of this DatafileInputV1.

        :return: The additional_properties of this DatafileInputV1.
        :rtype: list[ScalarPropertyV1]
        """
        return self._additional_properties

    @additional_properties.setter
    def additional_properties(self, additional_properties):
        """
        Sets the additional_properties of this DatafileInputV1.

        :param additional_properties: The additional_properties of this DatafileInputV1.
        :type: list[ScalarPropertyV1]
        """

        self._additional_properties = additional_properties

    @property
    def append(self):
        """
        Gets the append of this DatafileInputV1.
        If true, append the data in this CSV file to the signals or condition.

        :return: The append of this DatafileInputV1.
        :rtype: bool
        """
        return self._append

    @append.setter
    def append(self, append):
        """
        Sets the append of this DatafileInputV1.
        If true, append the data in this CSV file to the signals or condition.

        :param append: The append of this DatafileInputV1.
        :type: bool
        """

        self._append = append

    @property
    def condition_name(self):
        """
        Gets the condition_name of this DatafileInputV1.
        The name to use for the condition being imported. If this datafile already has a condition by this name, the import will modify the already existing condition rather than creating another condition with the same name. This setting is ignored when importing signals.

        :return: The condition_name of this DatafileInputV1.
        :rtype: str
        """
        return self._condition_name

    @condition_name.setter
    def condition_name(self, condition_name):
        """
        Sets the condition_name of this DatafileInputV1.
        The name to use for the condition being imported. If this datafile already has a condition by this name, the import will modify the already existing condition rather than creating another condition with the same name. This setting is ignored when importing signals.

        :param condition_name: The condition_name of this DatafileInputV1.
        :type: str
        """

        self._condition_name = condition_name

    @property
    def data_id(self):
        """
        Gets the data_id of this DatafileInputV1.
        The data ID of this item. Note: This is not the Seeq ID, but the unique identifier that the remote datasource uses.

        :return: The data_id of this DatafileInputV1.
        :rtype: str
        """
        return self._data_id

    @data_id.setter
    def data_id(self, data_id):
        """
        Sets the data_id of this DatafileInputV1.
        The data ID of this item. Note: This is not the Seeq ID, but the unique identifier that the remote datasource uses.

        :param data_id: The data_id of this DatafileInputV1.
        :type: str
        """

        self._data_id = data_id

    @property
    def day_first_default(self):
        """
        Gets the day_first_default of this DatafileInputV1.
        If true, assume day first dates when ambiguous. If false (default), assume month first dates when ambiguous. For example, 07/01/16 is ambiguous and could be a day first or month first date. This setting is only used when there is not enough information in the column to distinguish month first from day first dates.

        :return: The day_first_default of this DatafileInputV1.
        :rtype: bool
        """
        return self._day_first_default

    @day_first_default.setter
    def day_first_default(self, day_first_default):
        """
        Sets the day_first_default of this DatafileInputV1.
        If true, assume day first dates when ambiguous. If false (default), assume month first dates when ambiguous. For example, 07/01/16 is ambiguous and could be a day first or month first date. This setting is only used when there is not enough information in the column to distinguish month first from day first dates.

        :param day_first_default: The day_first_default of this DatafileInputV1.
        :type: bool
        """

        self._day_first_default = day_first_default

    @property
    def description(self):
        """
        Gets the description of this DatafileInputV1.
        Clarifying information or other plain language description of this item. An input of just whitespaces is equivalent to a null input.

        :return: The description of this DatafileInputV1.
        :rtype: str
        """
        return self._description

    @description.setter
    def description(self, description):
        """
        Sets the description of this DatafileInputV1.
        Clarifying information or other plain language description of this item. An input of just whitespaces is equivalent to a null input.

        :param description: The description of this DatafileInputV1.
        :type: str
        """

        self._description = description

    @property
    def description_row(self):
        """
        Gets the description_row of this DatafileInputV1.
        Integer that identifies the row containing the description for each signal. If there is no such row, set to 0. If not specified, defaults to 0. (Row 1 is the first row of the file.) If importing a condition, the content of this row is ignored.

        :return: The description_row of this DatafileInputV1.
        :rtype: int
        """
        return self._description_row

    @description_row.setter
    def description_row(self, description_row):
        """
        Sets the description_row of this DatafileInputV1.
        Integer that identifies the row containing the description for each signal. If there is no such row, set to 0. If not specified, defaults to 0. (Row 1 is the first row of the file.) If importing a condition, the content of this row is ignored.

        :param description_row: The description_row of this DatafileInputV1.
        :type: int
        """

        self._description_row = description_row

    @property
    def end_column_index(self):
        """
        Gets the end_column_index of this DatafileInputV1.
        Integer that identifies the column containing the capsule end key for the condition. If not specified, defaults to keyColumnIndex + 1. If endColumnName is specified, this setting will be ignored. If importing a signal, this setting is ignored.

        :return: The end_column_index of this DatafileInputV1.
        :rtype: int
        """
        return self._end_column_index

    @end_column_index.setter
    def end_column_index(self, end_column_index):
        """
        Sets the end_column_index of this DatafileInputV1.
        Integer that identifies the column containing the capsule end key for the condition. If not specified, defaults to keyColumnIndex + 1. If endColumnName is specified, this setting will be ignored. If importing a signal, this setting is ignored.

        :param end_column_index: The end_column_index of this DatafileInputV1.
        :type: int
        """

        self._end_column_index = end_column_index

    @property
    def end_column_name(self):
        """
        Gets the end_column_name of this DatafileInputV1.
        The name of the column containing the capsule end key for the condition. If not specified or whitespace, the endColumnIndex will be used. If specified, the endColumnIndex will be ignored.

        :return: The end_column_name of this DatafileInputV1.
        :rtype: str
        """
        return self._end_column_name

    @end_column_name.setter
    def end_column_name(self, end_column_name):
        """
        Sets the end_column_name of this DatafileInputV1.
        The name of the column containing the capsule end key for the condition. If not specified or whitespace, the endColumnIndex will be used. If specified, the endColumnIndex will be ignored.

        :param end_column_name: The end_column_name of this DatafileInputV1.
        :type: str
        """

        self._end_column_name = end_column_name

    @property
    def field_delimiter(self):
        """
        Gets the field_delimiter of this DatafileInputV1.
        The character used as the CSV field delimiter. The possibilities are comma, semicolon, and tab. If not specified, defaults to comma.

        :return: The field_delimiter of this DatafileInputV1.
        :rtype: str
        """
        return self._field_delimiter

    @field_delimiter.setter
    def field_delimiter(self, field_delimiter):
        """
        Sets the field_delimiter of this DatafileInputV1.
        The character used as the CSV field delimiter. The possibilities are comma, semicolon, and tab. If not specified, defaults to comma.

        :param field_delimiter: The field_delimiter of this DatafileInputV1.
        :type: str
        """
        allowed_values = ["Comma", "Semicolon", "Tab"]
        if field_delimiter not in allowed_values:
            raise ValueError(
                "Invalid value for `field_delimiter` ({0}), must be one of {1}"
                .format(field_delimiter, allowed_values)
            )

        self._field_delimiter = field_delimiter

    @property
    def filename(self):
        """
        Gets the filename of this DatafileInputV1.
        The name of the file that the client uploaded. Useful for keeping track of what file was used to create the Datafile.

        :return: The filename of this DatafileInputV1.
        :rtype: str
        """
        return self._filename

    @filename.setter
    def filename(self, filename):
        """
        Sets the filename of this DatafileInputV1.
        The name of the file that the client uploaded. Useful for keeping track of what file was used to create the Datafile.

        :param filename: The filename of this DatafileInputV1.
        :type: str
        """

        self._filename = filename

    @property
    def first_data_row(self):
        """
        Gets the first_data_row of this DatafileInputV1.
        Integer that identifies the row at which to start reading the data. If not specified, defaults to 2. (Row 1 is the first row of the file.)

        :return: The first_data_row of this DatafileInputV1.
        :rtype: int
        """
        return self._first_data_row

    @first_data_row.setter
    def first_data_row(self, first_data_row):
        """
        Sets the first_data_row of this DatafileInputV1.
        Integer that identifies the row at which to start reading the data. If not specified, defaults to 2. (Row 1 is the first row of the file.)

        :param first_data_row: The first_data_row of this DatafileInputV1.
        :type: int
        """

        self._first_data_row = first_data_row

    @property
    def host_id(self):
        """
        Gets the host_id of this DatafileInputV1.
        The ID of the datasource hosting this item. Note that this is a Seeq-generated ID, not the way that the datasource identifies itself.

        :return: The host_id of this DatafileInputV1.
        :rtype: str
        """
        return self._host_id

    @host_id.setter
    def host_id(self, host_id):
        """
        Sets the host_id of this DatafileInputV1.
        The ID of the datasource hosting this item. Note that this is a Seeq-generated ID, not the way that the datasource identifies itself.

        :param host_id: The host_id of this DatafileInputV1.
        :type: str
        """

        self._host_id = host_id

    @property
    def interpolation_method(self):
        """
        Gets the interpolation_method of this DatafileInputV1.
        The interpolation method used to represent the values between samples in the signal. The possibilities are: Linear, PILinear, and Step. If not specified, defaults to Linear. If an interpolation method row is specified, the information in that row overrides this setting. If importing a condition, this setting is ignored.

        :return: The interpolation_method of this DatafileInputV1.
        :rtype: str
        """
        return self._interpolation_method

    @interpolation_method.setter
    def interpolation_method(self, interpolation_method):
        """
        Sets the interpolation_method of this DatafileInputV1.
        The interpolation method used to represent the values between samples in the signal. The possibilities are: Linear, PILinear, and Step. If not specified, defaults to Linear. If an interpolation method row is specified, the information in that row overrides this setting. If importing a condition, this setting is ignored.

        :param interpolation_method: The interpolation_method of this DatafileInputV1.
        :type: str
        """

        self._interpolation_method = interpolation_method

    @property
    def interpolation_method_row(self):
        """
        Gets the interpolation_method_row of this DatafileInputV1.
        Integer that identifies the row containing the interpolation method for each signal. If there is no such row, set to 0. If not specified, defaults to 0. If an interpolation method in the row is not recognized, the import will fail. If an interpolation method in the row is blank, the interpolationMethod setting will be used as the default. (Row 1 is the first row of the file.)If importing a condition, the content of this row is ignored.

        :return: The interpolation_method_row of this DatafileInputV1.
        :rtype: int
        """
        return self._interpolation_method_row

    @interpolation_method_row.setter
    def interpolation_method_row(self, interpolation_method_row):
        """
        Sets the interpolation_method_row of this DatafileInputV1.
        Integer that identifies the row containing the interpolation method for each signal. If there is no such row, set to 0. If not specified, defaults to 0. If an interpolation method in the row is not recognized, the import will fail. If an interpolation method in the row is blank, the interpolationMethod setting will be used as the default. (Row 1 is the first row of the file.)If importing a condition, the content of this row is ignored.

        :param interpolation_method_row: The interpolation_method_row of this DatafileInputV1.
        :type: int
        """

        self._interpolation_method_row = interpolation_method_row

    @property
    def item_type(self):
        """
        Gets the item_type of this DatafileInputV1.
        The type of item to be imported from the CSV file. Supported types include signal and condition.

        :return: The item_type of this DatafileInputV1.
        :rtype: str
        """
        return self._item_type

    @item_type.setter
    def item_type(self, item_type):
        """
        Sets the item_type of this DatafileInputV1.
        The type of item to be imported from the CSV file. Supported types include signal and condition.

        :param item_type: The item_type of this DatafileInputV1.
        :type: str
        """
        allowed_values = ["Signal", "Condition", "SupportedItemType"]
        if item_type not in allowed_values:
            raise ValueError(
                "Invalid value for `item_type` ({0}), must be one of {1}"
                .format(item_type, allowed_values)
            )

        self._item_type = item_type

    @property
    def key_column_index(self):
        """
        Gets the key_column_index of this DatafileInputV1.
        Integer that identifies the column containing the sample timestamps for the signal(s) or the column containing the capsule start key for the condition. If not specified, defaults to 1, the first column. If keyColumnName is specified, this setting will be ignored.

        :return: The key_column_index of this DatafileInputV1.
        :rtype: int
        """
        return self._key_column_index

    @key_column_index.setter
    def key_column_index(self, key_column_index):
        """
        Sets the key_column_index of this DatafileInputV1.
        Integer that identifies the column containing the sample timestamps for the signal(s) or the column containing the capsule start key for the condition. If not specified, defaults to 1, the first column. If keyColumnName is specified, this setting will be ignored.

        :param key_column_index: The key_column_index of this DatafileInputV1.
        :type: int
        """

        self._key_column_index = key_column_index

    @property
    def key_column_name(self):
        """
        Gets the key_column_name of this DatafileInputV1.
        The name of the column containing the signal timestamps for the signal(s) or the column containing the capsule start key for the condition. If not specified or whitespace, the keyColumnIndex will be used. If specified, the keyColumnIndex will be ignored.

        :return: The key_column_name of this DatafileInputV1.
        :rtype: str
        """
        return self._key_column_name

    @key_column_name.setter
    def key_column_name(self, key_column_name):
        """
        Sets the key_column_name of this DatafileInputV1.
        The name of the column containing the signal timestamps for the signal(s) or the column containing the capsule start key for the condition. If not specified or whitespace, the keyColumnIndex will be used. If specified, the keyColumnIndex will be ignored.

        :param key_column_name: The key_column_name of this DatafileInputV1.
        :type: str
        """

        self._key_column_name = key_column_name

    @property
    def key_format(self):
        """
        Gets the key_format of this DatafileInputV1.
        The format of the sample timestamps for signals or the format of the capsule start and end times for a condition. If not specified, defaults to ISO8601.

        :return: The key_format of this DatafileInputV1.
        :rtype: str
        """
        return self._key_format

    @key_format.setter
    def key_format(self, key_format):
        """
        Sets the key_format of this DatafileInputV1.
        The format of the sample timestamps for signals or the format of the capsule start and end times for a condition. If not specified, defaults to ISO8601.

        :param key_format: The key_format of this DatafileInputV1.
        :type: str
        """
        allowed_values = ["ISO8601", "MONTH_DAY_YEAR_24HRCLOCK", "MONTH_DAY_YEAR_12HRCLOCK", "UNIX_EPOCH_SECONDS", "KeyFormatType"]
        if key_format not in allowed_values:
            raise ValueError(
                "Invalid value for `key_format` ({0}), must be one of {1}"
                .format(key_format, allowed_values)
            )

        self._key_format = key_format

    @property
    def lenient_daylight_savings(self):
        """
        Gets the lenient_daylight_savings of this DatafileInputV1.
        If true, hours are allowed that don't exist due to the spring forward daylight savings transition. They are interpreted as occurring in the following hour. The true setting should not be needed if the data was logged appropriately for its time zone. If false (default), data in hours that don't exist will cause the import to fail.

        :return: The lenient_daylight_savings of this DatafileInputV1.
        :rtype: bool
        """
        return self._lenient_daylight_savings

    @lenient_daylight_savings.setter
    def lenient_daylight_savings(self, lenient_daylight_savings):
        """
        Sets the lenient_daylight_savings of this DatafileInputV1.
        If true, hours are allowed that don't exist due to the spring forward daylight savings transition. They are interpreted as occurring in the following hour. The true setting should not be needed if the data was logged appropriately for its time zone. If false (default), data in hours that don't exist will cause the import to fail.

        :param lenient_daylight_savings: The lenient_daylight_savings of this DatafileInputV1.
        :type: bool
        """

        self._lenient_daylight_savings = lenient_daylight_savings

    @property
    def maximum_duration(self):
        """
        Gets the maximum_duration of this DatafileInputV1.
        The maximum duration of the capsules in the condition. Capsules greater than this duration will be imported but will not returned when data from the condition is requested. If importing a signal, this setting is ignored.

        :return: The maximum_duration of this DatafileInputV1.
        :rtype: str
        """
        return self._maximum_duration

    @maximum_duration.setter
    def maximum_duration(self, maximum_duration):
        """
        Sets the maximum_duration of this DatafileInputV1.
        The maximum duration of the capsules in the condition. Capsules greater than this duration will be imported but will not returned when data from the condition is requested. If importing a signal, this setting is ignored.

        :param maximum_duration: The maximum_duration of this DatafileInputV1.
        :type: str
        """

        self._maximum_duration = maximum_duration

    @property
    def maximum_interpolation(self):
        """
        Gets the maximum_interpolation of this DatafileInputV1.
        The maximum spacing between adjacent sample keys that can be interpolated across. If two samples are spaced by more than maximum interpolation, there will be a hole in the signal between them. If not specified, defaults to 40h. If a maximum interpolation row is specified, the information in that row overrides this setting. If importing a condition, this setting is ignored.

        :return: The maximum_interpolation of this DatafileInputV1.
        :rtype: str
        """
        return self._maximum_interpolation

    @maximum_interpolation.setter
    def maximum_interpolation(self, maximum_interpolation):
        """
        Sets the maximum_interpolation of this DatafileInputV1.
        The maximum spacing between adjacent sample keys that can be interpolated across. If two samples are spaced by more than maximum interpolation, there will be a hole in the signal between them. If not specified, defaults to 40h. If a maximum interpolation row is specified, the information in that row overrides this setting. If importing a condition, this setting is ignored.

        :param maximum_interpolation: The maximum_interpolation of this DatafileInputV1.
        :type: str
        """

        self._maximum_interpolation = maximum_interpolation

    @property
    def maximum_interpolation_row(self):
        """
        Gets the maximum_interpolation_row of this DatafileInputV1.
        Integer that identifies the row containing the maximum interpolation for each signal. If there is no such row, set to 0. If not specified, defaults to 0. If a maximum duration in the row is not recognized, the import will fail. If an maximum interpolation in the row is blank, the maximumInterpolation setting will be used as the default. (Row 1 is the first row of the file.)If importing a condition, the content of this row is ignored.

        :return: The maximum_interpolation_row of this DatafileInputV1.
        :rtype: int
        """
        return self._maximum_interpolation_row

    @maximum_interpolation_row.setter
    def maximum_interpolation_row(self, maximum_interpolation_row):
        """
        Sets the maximum_interpolation_row of this DatafileInputV1.
        Integer that identifies the row containing the maximum interpolation for each signal. If there is no such row, set to 0. If not specified, defaults to 0. If a maximum duration in the row is not recognized, the import will fail. If an maximum interpolation in the row is blank, the maximumInterpolation setting will be used as the default. (Row 1 is the first row of the file.)If importing a condition, the content of this row is ignored.

        :param maximum_interpolation_row: The maximum_interpolation_row of this DatafileInputV1.
        :type: int
        """

        self._maximum_interpolation_row = maximum_interpolation_row

    @property
    def name(self):
        """
        Gets the name of this DatafileInputV1.
        Human readable name. Required during creation. An input of just whitespaces is equivalent to a null input.

        :return: The name of this DatafileInputV1.
        :rtype: str
        """
        return self._name

    @name.setter
    def name(self, name):
        """
        Sets the name of this DatafileInputV1.
        Human readable name. Required during creation. An input of just whitespaces is equivalent to a null input.

        :param name: The name of this DatafileInputV1.
        :type: str
        """
        if name is None:
            raise ValueError("Invalid value for `name`, must not be `None`")

        self._name = name

    @property
    def name_prefix(self):
        """
        Gets the name_prefix of this DatafileInputV1.
        Prefix prepended to the name of each signal when importing signal(s) and prepended to the name of each capsule property when importing a condition. Leading whitespace is ignored.

        :return: The name_prefix of this DatafileInputV1.
        :rtype: str
        """
        return self._name_prefix

    @name_prefix.setter
    def name_prefix(self, name_prefix):
        """
        Sets the name_prefix of this DatafileInputV1.
        Prefix prepended to the name of each signal when importing signal(s) and prepended to the name of each capsule property when importing a condition. Leading whitespace is ignored.

        :param name_prefix: The name_prefix of this DatafileInputV1.
        :type: str
        """

        self._name_prefix = name_prefix

    @property
    def name_row(self):
        """
        Gets the name_row of this DatafileInputV1.
        Integer that identifies the header row used to name the signal(s) when importing signal(s) and used to name the capsule properties when importing a condition. If not specified, defaults to 1 (first row). The name row is required and must have unique non-whitespace entries.

        :return: The name_row of this DatafileInputV1.
        :rtype: int
        """
        return self._name_row

    @name_row.setter
    def name_row(self, name_row):
        """
        Sets the name_row of this DatafileInputV1.
        Integer that identifies the header row used to name the signal(s) when importing signal(s) and used to name the capsule properties when importing a condition. If not specified, defaults to 1 (first row). The name row is required and must have unique non-whitespace entries.

        :param name_row: The name_row of this DatafileInputV1.
        :type: int
        """

        self._name_row = name_row

    @property
    def name_suffix(self):
        """
        Gets the name_suffix of this DatafileInputV1.
        Suffix appended to the name of each signal when importing signal(s) and appended to the name of each capsule property when importing a condition. Trailing whitespace is ignored.

        :return: The name_suffix of this DatafileInputV1.
        :rtype: str
        """
        return self._name_suffix

    @name_suffix.setter
    def name_suffix(self, name_suffix):
        """
        Sets the name_suffix of this DatafileInputV1.
        Suffix appended to the name of each signal when importing signal(s) and appended to the name of each capsule property when importing a condition. Trailing whitespace is ignored.

        :param name_suffix: The name_suffix of this DatafileInputV1.
        :type: str
        """

        self._name_suffix = name_suffix

    @property
    def properties(self):
        """
        Gets the properties of this DatafileInputV1.

        :return: The properties of this DatafileInputV1.
        :rtype: list[ScalarPropertyV1]
        """
        return self._properties

    @properties.setter
    def properties(self, properties):
        """
        Sets the properties of this DatafileInputV1.

        :param properties: The properties of this DatafileInputV1.
        :type: list[ScalarPropertyV1]
        """

        self._properties = properties

    @property
    def scoped_to(self):
        """
        Gets the scoped_to of this DatafileInputV1.
        The ID of the workbook to which this item will be scoped.

        :return: The scoped_to of this DatafileInputV1.
        :rtype: str
        """
        return self._scoped_to

    @scoped_to.setter
    def scoped_to(self, scoped_to):
        """
        Sets the scoped_to of this DatafileInputV1.
        The ID of the workbook to which this item will be scoped.

        :param scoped_to: The scoped_to of this DatafileInputV1.
        :type: str
        """

        self._scoped_to = scoped_to

    @property
    def time_zone(self):
        """
        Gets the time_zone of this DatafileInputV1.
        If the timestamps (key for signals, start/end for a condition) contain no time zone information, they will be interpreted as being in this time zone.  If not specified and the timestamps contain no time zone information, the time zone of the Seeq server is used. If the timestamps contain time zone information, this setting is ignored.

        :return: The time_zone of this DatafileInputV1.
        :rtype: str
        """
        return self._time_zone

    @time_zone.setter
    def time_zone(self, time_zone):
        """
        Sets the time_zone of this DatafileInputV1.
        If the timestamps (key for signals, start/end for a condition) contain no time zone information, they will be interpreted as being in this time zone.  If not specified and the timestamps contain no time zone information, the time zone of the Seeq server is used. If the timestamps contain time zone information, this setting is ignored.

        :param time_zone: The time_zone of this DatafileInputV1.
        :type: str
        """

        self._time_zone = time_zone

    @property
    def upload_filename(self):
        """
        Gets the upload_filename of this DatafileInputV1.
        The server-side name of the CSV file that is returned from the upload endpoint.

        :return: The upload_filename of this DatafileInputV1.
        :rtype: str
        """
        return self._upload_filename

    @upload_filename.setter
    def upload_filename(self, upload_filename):
        """
        Sets the upload_filename of this DatafileInputV1.
        The server-side name of the CSV file that is returned from the upload endpoint.

        :param upload_filename: The upload_filename of this DatafileInputV1.
        :type: str
        """
        if upload_filename is None:
            raise ValueError("Invalid value for `upload_filename`, must not be `None`")

        self._upload_filename = upload_filename

    @property
    def validation_mode(self):
        """
        Gets the validation_mode of this DatafileInputV1.
        The approach to use when CSV data cannot be parsed. Choices are Fail, Skip, Invalid. If Fail (default), then cells that cannot be parsed will cause the import to fail with error messages.If Skip, those cells will be skipped meaning that no sample will be created for signals from that row of the file. For conditions, if it is the start or end cell, no capsule will be created from that row. If the cell is a capsule property, the capsule is still created but without that capsule property. If Invalid and the cell is a sample key or capsule start/end, no sample or capsule is created from that row of the file. If the cell is a sample value or capsule property, the sample or capsule property is created with the value INVALID.

        :return: The validation_mode of this DatafileInputV1.
        :rtype: str
        """
        return self._validation_mode

    @validation_mode.setter
    def validation_mode(self, validation_mode):
        """
        Sets the validation_mode of this DatafileInputV1.
        The approach to use when CSV data cannot be parsed. Choices are Fail, Skip, Invalid. If Fail (default), then cells that cannot be parsed will cause the import to fail with error messages.If Skip, those cells will be skipped meaning that no sample will be created for signals from that row of the file. For conditions, if it is the start or end cell, no capsule will be created from that row. If the cell is a capsule property, the capsule is still created but without that capsule property. If Invalid and the cell is a sample key or capsule start/end, no sample or capsule is created from that row of the file. If the cell is a sample value or capsule property, the sample or capsule property is created with the value INVALID.

        :param validation_mode: The validation_mode of this DatafileInputV1.
        :type: str
        """
        allowed_values = ["Fail", "Skip", "Invalid"]
        if validation_mode not in allowed_values:
            raise ValueError(
                "Invalid value for `validation_mode` ({0}), must be one of {1}"
                .format(validation_mode, allowed_values)
            )

        self._validation_mode = validation_mode

    @property
    def value_column_indices(self):
        """
        Gets the value_column_indices of this DatafileInputV1.
        List of integers identifying the columns to import. When importing signals, these columns will be combined with the key column to create signals. When importing a condition, these columns will become the capsule properties. Valid formats are a comma separated list of 'N' or 'N-M' where N and M are integers greater than zero and M >= N. Example: '2, 5-7, 10, 12-14'. The first column of the file is column 1. If the column(s) representing a signal key or condition start/end is included in the list, it will be ignored. If neither valueColumnNames nor valueColumnIndices are specified, all columns other than the key/start/end column will result in signals when importing signals and will result in capsule properties when importing a condition. An entry of 0 alone indicates that no columns should be imported as capsule properties. If a column in this list cannot be found in the file, the import will fail. Any column is only imported once no matter how many times it is listed.

        :return: The value_column_indices of this DatafileInputV1.
        :rtype: str
        """
        return self._value_column_indices

    @value_column_indices.setter
    def value_column_indices(self, value_column_indices):
        """
        Sets the value_column_indices of this DatafileInputV1.
        List of integers identifying the columns to import. When importing signals, these columns will be combined with the key column to create signals. When importing a condition, these columns will become the capsule properties. Valid formats are a comma separated list of 'N' or 'N-M' where N and M are integers greater than zero and M >= N. Example: '2, 5-7, 10, 12-14'. The first column of the file is column 1. If the column(s) representing a signal key or condition start/end is included in the list, it will be ignored. If neither valueColumnNames nor valueColumnIndices are specified, all columns other than the key/start/end column will result in signals when importing signals and will result in capsule properties when importing a condition. An entry of 0 alone indicates that no columns should be imported as capsule properties. If a column in this list cannot be found in the file, the import will fail. Any column is only imported once no matter how many times it is listed.

        :param value_column_indices: The value_column_indices of this DatafileInputV1.
        :type: str
        """

        self._value_column_indices = value_column_indices

    @property
    def value_column_names(self):
        """
        Gets the value_column_names of this DatafileInputV1.
        List of comma separated case sensitive names of the columns to import. When importing signals, these columns will be combined with the key column to create signals. When importing a condition, these columns will become the capsule properties. If the column(s) representing a signal key or condition start/end is included in the list, it will be ignored. If not specified, valueColumnIndices will be used. If specified, valueColumnIndices will be ignored. If a column name in this list cannot be found in the file, the import will fail. Any column is only imported once no matter how many times it is listed.

        :return: The value_column_names of this DatafileInputV1.
        :rtype: str
        """
        return self._value_column_names

    @value_column_names.setter
    def value_column_names(self, value_column_names):
        """
        Sets the value_column_names of this DatafileInputV1.
        List of comma separated case sensitive names of the columns to import. When importing signals, these columns will be combined with the key column to create signals. When importing a condition, these columns will become the capsule properties. If the column(s) representing a signal key or condition start/end is included in the list, it will be ignored. If not specified, valueColumnIndices will be used. If specified, valueColumnIndices will be ignored. If a column name in this list cannot be found in the file, the import will fail. Any column is only imported once no matter how many times it is listed.

        :param value_column_names: The value_column_names of this DatafileInputV1.
        :type: str
        """

        self._value_column_names = value_column_names

    @property
    def value_uom(self):
        """
        Gets the value_uom of this DatafileInputV1.
        The unit of measure to be used for every signal when importing signals and for every capsule property when importing a condition. If not specified, defaults to unitless. If a unit of measure row is specified, the information in that row overrides this setting. If this unit of measure is not recognized, the import will fail.

        :return: The value_uom of this DatafileInputV1.
        :rtype: str
        """
        return self._value_uom

    @value_uom.setter
    def value_uom(self, value_uom):
        """
        Sets the value_uom of this DatafileInputV1.
        The unit of measure to be used for every signal when importing signals and for every capsule property when importing a condition. If not specified, defaults to unitless. If a unit of measure row is specified, the information in that row overrides this setting. If this unit of measure is not recognized, the import will fail.

        :param value_uom: The value_uom of this DatafileInputV1.
        :type: str
        """

        self._value_uom = value_uom

    @property
    def value_uom_row(self):
        """
        Gets the value_uom_row of this DatafileInputV1.
        Integer that identifies the row containing the unit of measure for each signal when importing signal(s) or for each capsule property when importing a condition. If there is no such row, set to 0. If not specified, defaults to 0. If a unit of measure in the row is not recognized, unitless will be used instead. (Row 1 is the first row of the file.)

        :return: The value_uom_row of this DatafileInputV1.
        :rtype: int
        """
        return self._value_uom_row

    @value_uom_row.setter
    def value_uom_row(self, value_uom_row):
        """
        Sets the value_uom_row of this DatafileInputV1.
        Integer that identifies the row containing the unit of measure for each signal when importing signal(s) or for each capsule property when importing a condition. If there is no such row, set to 0. If not specified, defaults to 0. If a unit of measure in the row is not recognized, unitless will be used instead. (Row 1 is the first row of the file.)

        :param value_uom_row: The value_uom_row of this DatafileInputV1.
        :type: int
        """

        self._value_uom_row = value_uom_row

    def to_dict(self):
        """
        Returns the model properties as a dict
        """
        result = {}

        for attr, _ in iteritems(self.swagger_types):
            value = getattr(self, attr)
            if isinstance(value, list):
                result[attr] = list(map(
                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
                    value
                ))
            elif hasattr(value, "to_dict"):
                result[attr] = value.to_dict()
            elif isinstance(value, dict):
                result[attr] = dict(map(
                    lambda item: (item[0], item[1].to_dict())
                    if hasattr(item[1], "to_dict") else item,
                    value.items()
                ))
            else:
                result[attr] = value

        return result

    def to_str(self):
        """
        Returns the string representation of the model
        """
        return pformat(self.to_dict())

    def __repr__(self):
        """
        For `print` and `pprint`
        """
        return self.to_str()

    def __eq__(self, other):
        """
        Returns true if both objects are equal
        """
        if not isinstance(other, DatafileInputV1):
            return False

        return self.__dict__ == other.__dict__

    def __ne__(self, other):
        """
        Returns true if both objects are not equal
        """
        return not self == other
