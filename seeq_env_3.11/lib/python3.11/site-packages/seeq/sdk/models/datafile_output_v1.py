# coding: utf-8

"""
    Seeq REST API

    No description provided (generated by Swagger Codegen https://github.com/swagger-api/swagger-codegen)  # noqa: E501

    OpenAPI spec version: 66.4.0-v202412241102-CD
    
    Generated by: https://github.com/swagger-api/swagger-codegen.git
"""

from pprint import pformat
from six import iteritems
import re


class DatafileOutputV1(object):
    """
    NOTE: This class is auto generated by the swagger code generator program.
    Do not edit the class manually.
    """


    """
    Attributes:
      swagger_types (dict): The key is attribute name
                            and the value is attribute type.
      attribute_map (dict): The key is attribute name
                            and the value is json key in definition.
    """
    swagger_types = {
        'additional_properties': 'list[ScalarPropertyV1]',
        'append': 'bool',
        'condition_name': 'str',
        'created_at': 'str',
        'data_id': 'str',
        'datasource_class': 'str',
        'datasource_id': 'str',
        'day_first_default': 'bool',
        'description': 'str',
        'description_row': 'int',
        'effective_permissions': 'PermissionsV1',
        'end_column_index': 'int',
        'end_column_name': 'str',
        'field_delimiter': 'str',
        'filename': 'str',
        'first_data_row': 'int',
        'id': 'str',
        'interpolation_method': 'str',
        'interpolation_method_row': 'int',
        'is_archived': 'bool',
        'is_redacted': 'bool',
        'item_type': 'str',
        'key_column_index': 'int',
        'key_column_name': 'str',
        'key_format': 'str',
        'lenient_daylight_savings': 'bool',
        'maximum_duration': 'str',
        'maximum_interpolation': 'str',
        'maximum_interpolation_row': 'int',
        'name': 'str',
        'name_prefix': 'str',
        'name_row': 'int',
        'name_suffix': 'str',
        'scoped_to': 'str',
        'status_message': 'str',
        'time_zone': 'str',
        'translation_key': 'str',
        'type': 'str',
        'updated_at': 'str',
        'validation_mode': 'str',
        'value_column_indices': 'str',
        'value_column_names': 'str',
        'value_uom': 'str',
        'value_uom_row': 'int'
    }

    attribute_map = {
        'additional_properties': 'additionalProperties',
        'append': 'append',
        'condition_name': 'conditionName',
        'created_at': 'createdAt',
        'data_id': 'dataId',
        'datasource_class': 'datasourceClass',
        'datasource_id': 'datasourceId',
        'day_first_default': 'dayFirstDefault',
        'description': 'description',
        'description_row': 'descriptionRow',
        'effective_permissions': 'effectivePermissions',
        'end_column_index': 'endColumnIndex',
        'end_column_name': 'endColumnName',
        'field_delimiter': 'fieldDelimiter',
        'filename': 'filename',
        'first_data_row': 'firstDataRow',
        'id': 'id',
        'interpolation_method': 'interpolationMethod',
        'interpolation_method_row': 'interpolationMethodRow',
        'is_archived': 'isArchived',
        'is_redacted': 'isRedacted',
        'item_type': 'itemType',
        'key_column_index': 'keyColumnIndex',
        'key_column_name': 'keyColumnName',
        'key_format': 'keyFormat',
        'lenient_daylight_savings': 'lenientDaylightSavings',
        'maximum_duration': 'maximumDuration',
        'maximum_interpolation': 'maximumInterpolation',
        'maximum_interpolation_row': 'maximumInterpolationRow',
        'name': 'name',
        'name_prefix': 'namePrefix',
        'name_row': 'nameRow',
        'name_suffix': 'nameSuffix',
        'scoped_to': 'scopedTo',
        'status_message': 'statusMessage',
        'time_zone': 'timeZone',
        'translation_key': 'translationKey',
        'type': 'type',
        'updated_at': 'updatedAt',
        'validation_mode': 'validationMode',
        'value_column_indices': 'valueColumnIndices',
        'value_column_names': 'valueColumnNames',
        'value_uom': 'valueUom',
        'value_uom_row': 'valueUomRow'
    }

    def __init__(self, additional_properties=None, append=False, condition_name=None, created_at=None, data_id=None, datasource_class=None, datasource_id=None, day_first_default=False, description=None, description_row=None, effective_permissions=None, end_column_index=None, end_column_name=None, field_delimiter=None, filename=None, first_data_row=None, id=None, interpolation_method=None, interpolation_method_row=None, is_archived=False, is_redacted=False, item_type=None, key_column_index=None, key_column_name=None, key_format=None, lenient_daylight_savings=False, maximum_duration=None, maximum_interpolation=None, maximum_interpolation_row=None, name=None, name_prefix=None, name_row=None, name_suffix=None, scoped_to=None, status_message=None, time_zone=None, translation_key=None, type=None, updated_at=None, validation_mode=None, value_column_indices=None, value_column_names=None, value_uom=None, value_uom_row=None):
        """
        DatafileOutputV1 - a model defined in Swagger
        """

        self._additional_properties = None
        self._append = None
        self._condition_name = None
        self._created_at = None
        self._data_id = None
        self._datasource_class = None
        self._datasource_id = None
        self._day_first_default = None
        self._description = None
        self._description_row = None
        self._effective_permissions = None
        self._end_column_index = None
        self._end_column_name = None
        self._field_delimiter = None
        self._filename = None
        self._first_data_row = None
        self._id = None
        self._interpolation_method = None
        self._interpolation_method_row = None
        self._is_archived = None
        self._is_redacted = None
        self._item_type = None
        self._key_column_index = None
        self._key_column_name = None
        self._key_format = None
        self._lenient_daylight_savings = None
        self._maximum_duration = None
        self._maximum_interpolation = None
        self._maximum_interpolation_row = None
        self._name = None
        self._name_prefix = None
        self._name_row = None
        self._name_suffix = None
        self._scoped_to = None
        self._status_message = None
        self._time_zone = None
        self._translation_key = None
        self._type = None
        self._updated_at = None
        self._validation_mode = None
        self._value_column_indices = None
        self._value_column_names = None
        self._value_uom = None
        self._value_uom_row = None

        if additional_properties is not None:
          self.additional_properties = additional_properties
        if append is not None:
          self.append = append
        if condition_name is not None:
          self.condition_name = condition_name
        if created_at is not None:
          self.created_at = created_at
        if data_id is not None:
          self.data_id = data_id
        if datasource_class is not None:
          self.datasource_class = datasource_class
        if datasource_id is not None:
          self.datasource_id = datasource_id
        if day_first_default is not None:
          self.day_first_default = day_first_default
        if description is not None:
          self.description = description
        if description_row is not None:
          self.description_row = description_row
        if effective_permissions is not None:
          self.effective_permissions = effective_permissions
        if end_column_index is not None:
          self.end_column_index = end_column_index
        if end_column_name is not None:
          self.end_column_name = end_column_name
        if field_delimiter is not None:
          self.field_delimiter = field_delimiter
        if filename is not None:
          self.filename = filename
        if first_data_row is not None:
          self.first_data_row = first_data_row
        if id is not None:
          self.id = id
        if interpolation_method is not None:
          self.interpolation_method = interpolation_method
        if interpolation_method_row is not None:
          self.interpolation_method_row = interpolation_method_row
        if is_archived is not None:
          self.is_archived = is_archived
        if is_redacted is not None:
          self.is_redacted = is_redacted
        if item_type is not None:
          self.item_type = item_type
        if key_column_index is not None:
          self.key_column_index = key_column_index
        if key_column_name is not None:
          self.key_column_name = key_column_name
        if key_format is not None:
          self.key_format = key_format
        if lenient_daylight_savings is not None:
          self.lenient_daylight_savings = lenient_daylight_savings
        if maximum_duration is not None:
          self.maximum_duration = maximum_duration
        if maximum_interpolation is not None:
          self.maximum_interpolation = maximum_interpolation
        if maximum_interpolation_row is not None:
          self.maximum_interpolation_row = maximum_interpolation_row
        if name is not None:
          self.name = name
        if name_prefix is not None:
          self.name_prefix = name_prefix
        if name_row is not None:
          self.name_row = name_row
        if name_suffix is not None:
          self.name_suffix = name_suffix
        if scoped_to is not None:
          self.scoped_to = scoped_to
        if status_message is not None:
          self.status_message = status_message
        if time_zone is not None:
          self.time_zone = time_zone
        if translation_key is not None:
          self.translation_key = translation_key
        if type is not None:
          self.type = type
        if updated_at is not None:
          self.updated_at = updated_at
        if validation_mode is not None:
          self.validation_mode = validation_mode
        if value_column_indices is not None:
          self.value_column_indices = value_column_indices
        if value_column_names is not None:
          self.value_column_names = value_column_names
        if value_uom is not None:
          self.value_uom = value_uom
        if value_uom_row is not None:
          self.value_uom_row = value_uom_row

    @property
    def additional_properties(self):
        """
        Gets the additional_properties of this DatafileOutputV1.
        Additional properties of the item

        :return: The additional_properties of this DatafileOutputV1.
        :rtype: list[ScalarPropertyV1]
        """
        return self._additional_properties

    @additional_properties.setter
    def additional_properties(self, additional_properties):
        """
        Sets the additional_properties of this DatafileOutputV1.
        Additional properties of the item

        :param additional_properties: The additional_properties of this DatafileOutputV1.
        :type: list[ScalarPropertyV1]
        """

        self._additional_properties = additional_properties

    @property
    def append(self):
        """
        Gets the append of this DatafileOutputV1.
        If true, append the data in this CSV file to the signals or condition.

        :return: The append of this DatafileOutputV1.
        :rtype: bool
        """
        return self._append

    @append.setter
    def append(self, append):
        """
        Sets the append of this DatafileOutputV1.
        If true, append the data in this CSV file to the signals or condition.

        :param append: The append of this DatafileOutputV1.
        :type: bool
        """

        self._append = append

    @property
    def condition_name(self):
        """
        Gets the condition_name of this DatafileOutputV1.
        The name to use for the condition being imported. If this datafile already has a condition by this name, the import will modify the already existing condition rather than creating another condition with the same name. This setting is ignored when importing signals.

        :return: The condition_name of this DatafileOutputV1.
        :rtype: str
        """
        return self._condition_name

    @condition_name.setter
    def condition_name(self, condition_name):
        """
        Sets the condition_name of this DatafileOutputV1.
        The name to use for the condition being imported. If this datafile already has a condition by this name, the import will modify the already existing condition rather than creating another condition with the same name. This setting is ignored when importing signals.

        :param condition_name: The condition_name of this DatafileOutputV1.
        :type: str
        """

        self._condition_name = condition_name

    @property
    def created_at(self):
        """
        Gets the created_at of this DatafileOutputV1.
        The ISO 8601 date of when the datafile was created (YYYY-MM-DDThh:mm:ss.sssssssss±hh:mm)

        :return: The created_at of this DatafileOutputV1.
        :rtype: str
        """
        return self._created_at

    @created_at.setter
    def created_at(self, created_at):
        """
        Sets the created_at of this DatafileOutputV1.
        The ISO 8601 date of when the datafile was created (YYYY-MM-DDThh:mm:ss.sssssssss±hh:mm)

        :param created_at: The created_at of this DatafileOutputV1.
        :type: str
        """

        self._created_at = created_at

    @property
    def data_id(self):
        """
        Gets the data_id of this DatafileOutputV1.
        The data ID of this asset. Note: This is not the Seeq ID, but the unique identifier that the remote datasource uses.

        :return: The data_id of this DatafileOutputV1.
        :rtype: str
        """
        return self._data_id

    @data_id.setter
    def data_id(self, data_id):
        """
        Sets the data_id of this DatafileOutputV1.
        The data ID of this asset. Note: This is not the Seeq ID, but the unique identifier that the remote datasource uses.

        :param data_id: The data_id of this DatafileOutputV1.
        :type: str
        """

        self._data_id = data_id

    @property
    def datasource_class(self):
        """
        Gets the datasource_class of this DatafileOutputV1.
        The datasource class, which is the type of system holding the item, such as OSIsoft PI

        :return: The datasource_class of this DatafileOutputV1.
        :rtype: str
        """
        return self._datasource_class

    @datasource_class.setter
    def datasource_class(self, datasource_class):
        """
        Sets the datasource_class of this DatafileOutputV1.
        The datasource class, which is the type of system holding the item, such as OSIsoft PI

        :param datasource_class: The datasource_class of this DatafileOutputV1.
        :type: str
        """

        self._datasource_class = datasource_class

    @property
    def datasource_id(self):
        """
        Gets the datasource_id of this DatafileOutputV1.
        The datasource identifier, which is how the datasource holding this item identifies itself

        :return: The datasource_id of this DatafileOutputV1.
        :rtype: str
        """
        return self._datasource_id

    @datasource_id.setter
    def datasource_id(self, datasource_id):
        """
        Sets the datasource_id of this DatafileOutputV1.
        The datasource identifier, which is how the datasource holding this item identifies itself

        :param datasource_id: The datasource_id of this DatafileOutputV1.
        :type: str
        """

        self._datasource_id = datasource_id

    @property
    def day_first_default(self):
        """
        Gets the day_first_default of this DatafileOutputV1.
        If true, assume day first dates when ambiguous. If false (default), assume month first dates when ambiguous. For example, 07/01/16 is ambiguous and could be a day first or month first date. This setting is only used when there is not enough information in the column to distinguish month first from day first dates.

        :return: The day_first_default of this DatafileOutputV1.
        :rtype: bool
        """
        return self._day_first_default

    @day_first_default.setter
    def day_first_default(self, day_first_default):
        """
        Sets the day_first_default of this DatafileOutputV1.
        If true, assume day first dates when ambiguous. If false (default), assume month first dates when ambiguous. For example, 07/01/16 is ambiguous and could be a day first or month first date. This setting is only used when there is not enough information in the column to distinguish month first from day first dates.

        :param day_first_default: The day_first_default of this DatafileOutputV1.
        :type: bool
        """

        self._day_first_default = day_first_default

    @property
    def description(self):
        """
        Gets the description of this DatafileOutputV1.
        Clarifying information or other plain language description of this item

        :return: The description of this DatafileOutputV1.
        :rtype: str
        """
        return self._description

    @description.setter
    def description(self, description):
        """
        Sets the description of this DatafileOutputV1.
        Clarifying information or other plain language description of this item

        :param description: The description of this DatafileOutputV1.
        :type: str
        """

        self._description = description

    @property
    def description_row(self):
        """
        Gets the description_row of this DatafileOutputV1.
        Integer that identifies the row containing the description for each signal. A setting of 0 indicates that there is no description header row. (Row 1 is the first row of the file.) If importing a condition, the content of this row is ignored.

        :return: The description_row of this DatafileOutputV1.
        :rtype: int
        """
        return self._description_row

    @description_row.setter
    def description_row(self, description_row):
        """
        Sets the description_row of this DatafileOutputV1.
        Integer that identifies the row containing the description for each signal. A setting of 0 indicates that there is no description header row. (Row 1 is the first row of the file.) If importing a condition, the content of this row is ignored.

        :param description_row: The description_row of this DatafileOutputV1.
        :type: int
        """

        self._description_row = description_row

    @property
    def effective_permissions(self):
        """
        Gets the effective_permissions of this DatafileOutputV1.

        :return: The effective_permissions of this DatafileOutputV1.
        :rtype: PermissionsV1
        """
        return self._effective_permissions

    @effective_permissions.setter
    def effective_permissions(self, effective_permissions):
        """
        Sets the effective_permissions of this DatafileOutputV1.

        :param effective_permissions: The effective_permissions of this DatafileOutputV1.
        :type: PermissionsV1
        """

        self._effective_permissions = effective_permissions

    @property
    def end_column_index(self):
        """
        Gets the end_column_index of this DatafileOutputV1.
        Integer that identifies the column containing the capsule end key for the condition. If importing a signal, this setting is ignored.

        :return: The end_column_index of this DatafileOutputV1.
        :rtype: int
        """
        return self._end_column_index

    @end_column_index.setter
    def end_column_index(self, end_column_index):
        """
        Sets the end_column_index of this DatafileOutputV1.
        Integer that identifies the column containing the capsule end key for the condition. If importing a signal, this setting is ignored.

        :param end_column_index: The end_column_index of this DatafileOutputV1.
        :type: int
        """

        self._end_column_index = end_column_index

    @property
    def end_column_name(self):
        """
        Gets the end_column_name of this DatafileOutputV1.
        The name of the column containing the capsule end key for the condition. If not specified or whitespace, the endColumnIndex will be used.

        :return: The end_column_name of this DatafileOutputV1.
        :rtype: str
        """
        return self._end_column_name

    @end_column_name.setter
    def end_column_name(self, end_column_name):
        """
        Sets the end_column_name of this DatafileOutputV1.
        The name of the column containing the capsule end key for the condition. If not specified or whitespace, the endColumnIndex will be used.

        :param end_column_name: The end_column_name of this DatafileOutputV1.
        :type: str
        """

        self._end_column_name = end_column_name

    @property
    def field_delimiter(self):
        """
        Gets the field_delimiter of this DatafileOutputV1.
        The character used as the CSV field delimiter.

        :return: The field_delimiter of this DatafileOutputV1.
        :rtype: str
        """
        return self._field_delimiter

    @field_delimiter.setter
    def field_delimiter(self, field_delimiter):
        """
        Sets the field_delimiter of this DatafileOutputV1.
        The character used as the CSV field delimiter.

        :param field_delimiter: The field_delimiter of this DatafileOutputV1.
        :type: str
        """
        allowed_values = ["Comma", "Semicolon", "Tab"]
        if field_delimiter not in allowed_values:
            raise ValueError(
                "Invalid value for `field_delimiter` ({0}), must be one of {1}"
                .format(field_delimiter, allowed_values)
            )

        self._field_delimiter = field_delimiter

    @property
    def filename(self):
        """
        Gets the filename of this DatafileOutputV1.
        The name and path of the CSV file used to generate the content

        :return: The filename of this DatafileOutputV1.
        :rtype: str
        """
        return self._filename

    @filename.setter
    def filename(self, filename):
        """
        Sets the filename of this DatafileOutputV1.
        The name and path of the CSV file used to generate the content

        :param filename: The filename of this DatafileOutputV1.
        :type: str
        """

        self._filename = filename

    @property
    def first_data_row(self):
        """
        Gets the first_data_row of this DatafileOutputV1.
        Integer that identifies the row at which to start reading the data. (Row 1 is the first row of the file.)

        :return: The first_data_row of this DatafileOutputV1.
        :rtype: int
        """
        return self._first_data_row

    @first_data_row.setter
    def first_data_row(self, first_data_row):
        """
        Sets the first_data_row of this DatafileOutputV1.
        Integer that identifies the row at which to start reading the data. (Row 1 is the first row of the file.)

        :param first_data_row: The first_data_row of this DatafileOutputV1.
        :type: int
        """

        self._first_data_row = first_data_row

    @property
    def id(self):
        """
        Gets the id of this DatafileOutputV1.
        The ID that can be used to interact with the item

        :return: The id of this DatafileOutputV1.
        :rtype: str
        """
        return self._id

    @id.setter
    def id(self, id):
        """
        Sets the id of this DatafileOutputV1.
        The ID that can be used to interact with the item

        :param id: The id of this DatafileOutputV1.
        :type: str
        """
        if id is None:
            raise ValueError("Invalid value for `id`, must not be `None`")

        self._id = id

    @property
    def interpolation_method(self):
        """
        Gets the interpolation_method of this DatafileOutputV1.
        The interpolation method used to represent the values between samples in the signal. If a maximum interpolation row is specified, the information in that row overrides this setting. If importing a condition, this setting is ignored.

        :return: The interpolation_method of this DatafileOutputV1.
        :rtype: str
        """
        return self._interpolation_method

    @interpolation_method.setter
    def interpolation_method(self, interpolation_method):
        """
        Sets the interpolation_method of this DatafileOutputV1.
        The interpolation method used to represent the values between samples in the signal. If a maximum interpolation row is specified, the information in that row overrides this setting. If importing a condition, this setting is ignored.

        :param interpolation_method: The interpolation_method of this DatafileOutputV1.
        :type: str
        """

        self._interpolation_method = interpolation_method

    @property
    def interpolation_method_row(self):
        """
        Gets the interpolation_method_row of this DatafileOutputV1.
        Integer that identifies the row containing the interpolation method for each signal. A setting of 0 indicates that there is no interpolation method header row. (Row 1 is the first row of the file.) If importing a condition, the content of this row is ignored.

        :return: The interpolation_method_row of this DatafileOutputV1.
        :rtype: int
        """
        return self._interpolation_method_row

    @interpolation_method_row.setter
    def interpolation_method_row(self, interpolation_method_row):
        """
        Sets the interpolation_method_row of this DatafileOutputV1.
        Integer that identifies the row containing the interpolation method for each signal. A setting of 0 indicates that there is no interpolation method header row. (Row 1 is the first row of the file.) If importing a condition, the content of this row is ignored.

        :param interpolation_method_row: The interpolation_method_row of this DatafileOutputV1.
        :type: int
        """

        self._interpolation_method_row = interpolation_method_row

    @property
    def is_archived(self):
        """
        Gets the is_archived of this DatafileOutputV1.
        Whether item is archived

        :return: The is_archived of this DatafileOutputV1.
        :rtype: bool
        """
        return self._is_archived

    @is_archived.setter
    def is_archived(self, is_archived):
        """
        Sets the is_archived of this DatafileOutputV1.
        Whether item is archived

        :param is_archived: The is_archived of this DatafileOutputV1.
        :type: bool
        """

        self._is_archived = is_archived

    @property
    def is_redacted(self):
        """
        Gets the is_redacted of this DatafileOutputV1.
        Whether item is redacted

        :return: The is_redacted of this DatafileOutputV1.
        :rtype: bool
        """
        return self._is_redacted

    @is_redacted.setter
    def is_redacted(self, is_redacted):
        """
        Sets the is_redacted of this DatafileOutputV1.
        Whether item is redacted

        :param is_redacted: The is_redacted of this DatafileOutputV1.
        :type: bool
        """

        self._is_redacted = is_redacted

    @property
    def item_type(self):
        """
        Gets the item_type of this DatafileOutputV1.
        The type of item to be imported from the CSV file. Supported types include signal and condition.

        :return: The item_type of this DatafileOutputV1.
        :rtype: str
        """
        return self._item_type

    @item_type.setter
    def item_type(self, item_type):
        """
        Sets the item_type of this DatafileOutputV1.
        The type of item to be imported from the CSV file. Supported types include signal and condition.

        :param item_type: The item_type of this DatafileOutputV1.
        :type: str
        """
        allowed_values = ["Signal", "Condition", "SupportedItemType"]
        if item_type not in allowed_values:
            raise ValueError(
                "Invalid value for `item_type` ({0}), must be one of {1}"
                .format(item_type, allowed_values)
            )

        self._item_type = item_type

    @property
    def key_column_index(self):
        """
        Gets the key_column_index of this DatafileOutputV1.
        Integer that identifies the column containing the sample timestamps for the signal(s) or the column containing the capsule start key for the condition. Column 1 is the first column of the file.

        :return: The key_column_index of this DatafileOutputV1.
        :rtype: int
        """
        return self._key_column_index

    @key_column_index.setter
    def key_column_index(self, key_column_index):
        """
        Sets the key_column_index of this DatafileOutputV1.
        Integer that identifies the column containing the sample timestamps for the signal(s) or the column containing the capsule start key for the condition. Column 1 is the first column of the file.

        :param key_column_index: The key_column_index of this DatafileOutputV1.
        :type: int
        """

        self._key_column_index = key_column_index

    @property
    def key_column_name(self):
        """
        Gets the key_column_name of this DatafileOutputV1.
        The name of the column containing the signal timestamps for the signal(s) orthe column containing the capsule start key for the condition. If not specified or whitespace, the keyColumnIndex will be used.

        :return: The key_column_name of this DatafileOutputV1.
        :rtype: str
        """
        return self._key_column_name

    @key_column_name.setter
    def key_column_name(self, key_column_name):
        """
        Sets the key_column_name of this DatafileOutputV1.
        The name of the column containing the signal timestamps for the signal(s) orthe column containing the capsule start key for the condition. If not specified or whitespace, the keyColumnIndex will be used.

        :param key_column_name: The key_column_name of this DatafileOutputV1.
        :type: str
        """

        self._key_column_name = key_column_name

    @property
    def key_format(self):
        """
        Gets the key_format of this DatafileOutputV1.
        The format of the sample timestamps for signals or the format of the capsule start and end times for a condition.

        :return: The key_format of this DatafileOutputV1.
        :rtype: str
        """
        return self._key_format

    @key_format.setter
    def key_format(self, key_format):
        """
        Sets the key_format of this DatafileOutputV1.
        The format of the sample timestamps for signals or the format of the capsule start and end times for a condition.

        :param key_format: The key_format of this DatafileOutputV1.
        :type: str
        """
        allowed_values = ["ISO8601", "MONTH_DAY_YEAR_24HRCLOCK", "MONTH_DAY_YEAR_12HRCLOCK", "UNIX_EPOCH_SECONDS", "KeyFormatType"]
        if key_format not in allowed_values:
            raise ValueError(
                "Invalid value for `key_format` ({0}), must be one of {1}"
                .format(key_format, allowed_values)
            )

        self._key_format = key_format

    @property
    def lenient_daylight_savings(self):
        """
        Gets the lenient_daylight_savings of this DatafileOutputV1.
        If true, hours are allowed that don't exist due to the spring forward daylight savings transition. They are interpreted as occurring in the following hour. The true setting should not be needed if the data was logged appropriately for its time zone. If false (default), data in hours that don't exist will cause the import to fail.

        :return: The lenient_daylight_savings of this DatafileOutputV1.
        :rtype: bool
        """
        return self._lenient_daylight_savings

    @lenient_daylight_savings.setter
    def lenient_daylight_savings(self, lenient_daylight_savings):
        """
        Sets the lenient_daylight_savings of this DatafileOutputV1.
        If true, hours are allowed that don't exist due to the spring forward daylight savings transition. They are interpreted as occurring in the following hour. The true setting should not be needed if the data was logged appropriately for its time zone. If false (default), data in hours that don't exist will cause the import to fail.

        :param lenient_daylight_savings: The lenient_daylight_savings of this DatafileOutputV1.
        :type: bool
        """

        self._lenient_daylight_savings = lenient_daylight_savings

    @property
    def maximum_duration(self):
        """
        Gets the maximum_duration of this DatafileOutputV1.
        The maximum duration of the capsules in the condition. Capsules greater than this duration will be imported but will not returned when data from the condition is requested. If importing a signal, this setting is ignored.

        :return: The maximum_duration of this DatafileOutputV1.
        :rtype: str
        """
        return self._maximum_duration

    @maximum_duration.setter
    def maximum_duration(self, maximum_duration):
        """
        Sets the maximum_duration of this DatafileOutputV1.
        The maximum duration of the capsules in the condition. Capsules greater than this duration will be imported but will not returned when data from the condition is requested. If importing a signal, this setting is ignored.

        :param maximum_duration: The maximum_duration of this DatafileOutputV1.
        :type: str
        """

        self._maximum_duration = maximum_duration

    @property
    def maximum_interpolation(self):
        """
        Gets the maximum_interpolation of this DatafileOutputV1.
        The maximum spacing between adjacent sample keys that can be interpolated across. If two samples are spaced by more than maximum interpolation, there will be a hole in the signal between them. If a maximum interpolation row is specified, the information in that row overrides this setting. If importing a condition, this setting is ignored.

        :return: The maximum_interpolation of this DatafileOutputV1.
        :rtype: str
        """
        return self._maximum_interpolation

    @maximum_interpolation.setter
    def maximum_interpolation(self, maximum_interpolation):
        """
        Sets the maximum_interpolation of this DatafileOutputV1.
        The maximum spacing between adjacent sample keys that can be interpolated across. If two samples are spaced by more than maximum interpolation, there will be a hole in the signal between them. If a maximum interpolation row is specified, the information in that row overrides this setting. If importing a condition, this setting is ignored.

        :param maximum_interpolation: The maximum_interpolation of this DatafileOutputV1.
        :type: str
        """

        self._maximum_interpolation = maximum_interpolation

    @property
    def maximum_interpolation_row(self):
        """
        Gets the maximum_interpolation_row of this DatafileOutputV1.
        Integer that identifies the row containing tje maximum interpolation for each signal. A setting of 0 indicates that there is no maximum interpolation header row. (Row 1 is the first row of the file.)If importing a condition, the content of this row is ignored.

        :return: The maximum_interpolation_row of this DatafileOutputV1.
        :rtype: int
        """
        return self._maximum_interpolation_row

    @maximum_interpolation_row.setter
    def maximum_interpolation_row(self, maximum_interpolation_row):
        """
        Sets the maximum_interpolation_row of this DatafileOutputV1.
        Integer that identifies the row containing tje maximum interpolation for each signal. A setting of 0 indicates that there is no maximum interpolation header row. (Row 1 is the first row of the file.)If importing a condition, the content of this row is ignored.

        :param maximum_interpolation_row: The maximum_interpolation_row of this DatafileOutputV1.
        :type: int
        """

        self._maximum_interpolation_row = maximum_interpolation_row

    @property
    def name(self):
        """
        Gets the name of this DatafileOutputV1.
        The human readable name

        :return: The name of this DatafileOutputV1.
        :rtype: str
        """
        return self._name

    @name.setter
    def name(self, name):
        """
        Sets the name of this DatafileOutputV1.
        The human readable name

        :param name: The name of this DatafileOutputV1.
        :type: str
        """
        if name is None:
            raise ValueError("Invalid value for `name`, must not be `None`")

        self._name = name

    @property
    def name_prefix(self):
        """
        Gets the name_prefix of this DatafileOutputV1.
        Prefix prepended to the name of each signal when importing signal(s) and prepended to the name of each capsule property when importing a condition.

        :return: The name_prefix of this DatafileOutputV1.
        :rtype: str
        """
        return self._name_prefix

    @name_prefix.setter
    def name_prefix(self, name_prefix):
        """
        Sets the name_prefix of this DatafileOutputV1.
        Prefix prepended to the name of each signal when importing signal(s) and prepended to the name of each capsule property when importing a condition.

        :param name_prefix: The name_prefix of this DatafileOutputV1.
        :type: str
        """

        self._name_prefix = name_prefix

    @property
    def name_row(self):
        """
        Gets the name_row of this DatafileOutputV1.
        Integer that identifies the header row used to name the signal(s) when importing signal(s) and used to name the capsule properties when importing a condition. A setting of 0 indicates that there is no name header row. (Row 1 is the first row of the file.)

        :return: The name_row of this DatafileOutputV1.
        :rtype: int
        """
        return self._name_row

    @name_row.setter
    def name_row(self, name_row):
        """
        Sets the name_row of this DatafileOutputV1.
        Integer that identifies the header row used to name the signal(s) when importing signal(s) and used to name the capsule properties when importing a condition. A setting of 0 indicates that there is no name header row. (Row 1 is the first row of the file.)

        :param name_row: The name_row of this DatafileOutputV1.
        :type: int
        """

        self._name_row = name_row

    @property
    def name_suffix(self):
        """
        Gets the name_suffix of this DatafileOutputV1.
        Suffix appended to the name of each signal when importing signal(s) and appended to the name of each capsule property when importing a condition.

        :return: The name_suffix of this DatafileOutputV1.
        :rtype: str
        """
        return self._name_suffix

    @name_suffix.setter
    def name_suffix(self, name_suffix):
        """
        Sets the name_suffix of this DatafileOutputV1.
        Suffix appended to the name of each signal when importing signal(s) and appended to the name of each capsule property when importing a condition.

        :param name_suffix: The name_suffix of this DatafileOutputV1.
        :type: str
        """

        self._name_suffix = name_suffix

    @property
    def scoped_to(self):
        """
        Gets the scoped_to of this DatafileOutputV1.
        The ID of the workbook to which this item is scoped or null if it is in the global scope.

        :return: The scoped_to of this DatafileOutputV1.
        :rtype: str
        """
        return self._scoped_to

    @scoped_to.setter
    def scoped_to(self, scoped_to):
        """
        Sets the scoped_to of this DatafileOutputV1.
        The ID of the workbook to which this item is scoped or null if it is in the global scope.

        :param scoped_to: The scoped_to of this DatafileOutputV1.
        :type: str
        """

        self._scoped_to = scoped_to

    @property
    def status_message(self):
        """
        Gets the status_message of this DatafileOutputV1.
        A plain language status message with information about any issues that may have been encountered during an operation

        :return: The status_message of this DatafileOutputV1.
        :rtype: str
        """
        return self._status_message

    @status_message.setter
    def status_message(self, status_message):
        """
        Sets the status_message of this DatafileOutputV1.
        A plain language status message with information about any issues that may have been encountered during an operation

        :param status_message: The status_message of this DatafileOutputV1.
        :type: str
        """

        self._status_message = status_message

    @property
    def time_zone(self):
        """
        Gets the time_zone of this DatafileOutputV1.
        If the timestamps (key for signals, start/end for a condition) contain no time zone information, they will be interpreted as being in this time zone. 

        :return: The time_zone of this DatafileOutputV1.
        :rtype: str
        """
        return self._time_zone

    @time_zone.setter
    def time_zone(self, time_zone):
        """
        Sets the time_zone of this DatafileOutputV1.
        If the timestamps (key for signals, start/end for a condition) contain no time zone information, they will be interpreted as being in this time zone. 

        :param time_zone: The time_zone of this DatafileOutputV1.
        :type: str
        """

        self._time_zone = time_zone

    @property
    def translation_key(self):
        """
        Gets the translation_key of this DatafileOutputV1.
        The item's translation key, if any

        :return: The translation_key of this DatafileOutputV1.
        :rtype: str
        """
        return self._translation_key

    @translation_key.setter
    def translation_key(self, translation_key):
        """
        Sets the translation_key of this DatafileOutputV1.
        The item's translation key, if any

        :param translation_key: The translation_key of this DatafileOutputV1.
        :type: str
        """

        self._translation_key = translation_key

    @property
    def type(self):
        """
        Gets the type of this DatafileOutputV1.
        The type of the item

        :return: The type of this DatafileOutputV1.
        :rtype: str
        """
        return self._type

    @type.setter
    def type(self, type):
        """
        Sets the type of this DatafileOutputV1.
        The type of the item

        :param type: The type of this DatafileOutputV1.
        :type: str
        """
        if type is None:
            raise ValueError("Invalid value for `type`, must not be `None`")

        self._type = type

    @property
    def updated_at(self):
        """
        Gets the updated_at of this DatafileOutputV1.
        The ISO 8601 date of when the datafile was updated (YYYY-MM-DDThh:mm:ss.sssssssss±hh:mm)

        :return: The updated_at of this DatafileOutputV1.
        :rtype: str
        """
        return self._updated_at

    @updated_at.setter
    def updated_at(self, updated_at):
        """
        Sets the updated_at of this DatafileOutputV1.
        The ISO 8601 date of when the datafile was updated (YYYY-MM-DDThh:mm:ss.sssssssss±hh:mm)

        :param updated_at: The updated_at of this DatafileOutputV1.
        :type: str
        """

        self._updated_at = updated_at

    @property
    def validation_mode(self):
        """
        Gets the validation_mode of this DatafileOutputV1.
        The approach to use when CSV data cannot be parsed. If Fail (default), then cells that cannot be parsed will cause the import to fail with error messages.If Skip, those cells will be skipped meaning that no sample will be created for signals from that row of the file. For conditions, if it is the start or end cell, no capsule will be created from that row. If the cell is a capsule property, the capsule is still created but without that capsule property. If Invalid and the cell is a sample key or capsule start/end, no sample or capsule is created from that row of the file. If the cell is a sample value or capsule property, the sample or capsule property is created with the value INVALID.

        :return: The validation_mode of this DatafileOutputV1.
        :rtype: str
        """
        return self._validation_mode

    @validation_mode.setter
    def validation_mode(self, validation_mode):
        """
        Sets the validation_mode of this DatafileOutputV1.
        The approach to use when CSV data cannot be parsed. If Fail (default), then cells that cannot be parsed will cause the import to fail with error messages.If Skip, those cells will be skipped meaning that no sample will be created for signals from that row of the file. For conditions, if it is the start or end cell, no capsule will be created from that row. If the cell is a capsule property, the capsule is still created but without that capsule property. If Invalid and the cell is a sample key or capsule start/end, no sample or capsule is created from that row of the file. If the cell is a sample value or capsule property, the sample or capsule property is created with the value INVALID.

        :param validation_mode: The validation_mode of this DatafileOutputV1.
        :type: str
        """
        allowed_values = ["Fail", "Skip", "Invalid"]
        if validation_mode not in allowed_values:
            raise ValueError(
                "Invalid value for `validation_mode` ({0}), must be one of {1}"
                .format(validation_mode, allowed_values)
            )

        self._validation_mode = validation_mode

    @property
    def value_column_indices(self):
        """
        Gets the value_column_indices of this DatafileOutputV1.
        List of integers identifying columns. When importing signals, these columns will be combined with the key column to create signals. When importing a condition, these columns will become the capsule properties. Valid formats are a comma separated list of 'N' or 'N-M' where N and M are integers greater than zero and M >= N. Example: '2, 5-7, 10, 12-14'. The first column of the file is column 1. If the column(s) representing a signal key or condition start/end is included in the list, it will be ignored. If neither valueColumnNames nor valueColumnIndices are specified, all columns other than the key/start/end column will result in signals when importing signals and will result in capsule properties when importing a condition. An entry of 0 alone indicates that no columns should be imported as capsule properties. Any column is only imported once no matter how many times it is listed.

        :return: The value_column_indices of this DatafileOutputV1.
        :rtype: str
        """
        return self._value_column_indices

    @value_column_indices.setter
    def value_column_indices(self, value_column_indices):
        """
        Sets the value_column_indices of this DatafileOutputV1.
        List of integers identifying columns. When importing signals, these columns will be combined with the key column to create signals. When importing a condition, these columns will become the capsule properties. Valid formats are a comma separated list of 'N' or 'N-M' where N and M are integers greater than zero and M >= N. Example: '2, 5-7, 10, 12-14'. The first column of the file is column 1. If the column(s) representing a signal key or condition start/end is included in the list, it will be ignored. If neither valueColumnNames nor valueColumnIndices are specified, all columns other than the key/start/end column will result in signals when importing signals and will result in capsule properties when importing a condition. An entry of 0 alone indicates that no columns should be imported as capsule properties. Any column is only imported once no matter how many times it is listed.

        :param value_column_indices: The value_column_indices of this DatafileOutputV1.
        :type: str
        """

        self._value_column_indices = value_column_indices

    @property
    def value_column_names(self):
        """
        Gets the value_column_names of this DatafileOutputV1.
        List of comma separated case sensitive names of the columns. When importing signals, these columns will be combined with the key column to create signals. When importing a condition, these columns will become the capsule properties. If the column(s) representing a signal key or condition start/end is included in the list, it will be ignored. If not specified, valueColumnIndices will be used. If specified, valueColumnIndices will be ignored. Any column is only imported once no matter how many times it is listed.

        :return: The value_column_names of this DatafileOutputV1.
        :rtype: str
        """
        return self._value_column_names

    @value_column_names.setter
    def value_column_names(self, value_column_names):
        """
        Sets the value_column_names of this DatafileOutputV1.
        List of comma separated case sensitive names of the columns. When importing signals, these columns will be combined with the key column to create signals. When importing a condition, these columns will become the capsule properties. If the column(s) representing a signal key or condition start/end is included in the list, it will be ignored. If not specified, valueColumnIndices will be used. If specified, valueColumnIndices will be ignored. Any column is only imported once no matter how many times it is listed.

        :param value_column_names: The value_column_names of this DatafileOutputV1.
        :type: str
        """

        self._value_column_names = value_column_names

    @property
    def value_uom(self):
        """
        Gets the value_uom of this DatafileOutputV1.
        The unit of measure to be used for every signal when importing signals and for every capsule property when importing a condition. If not specified, defaults to unitless. If a unit of measure row is specified, the information in that row overrides this setting.

        :return: The value_uom of this DatafileOutputV1.
        :rtype: str
        """
        return self._value_uom

    @value_uom.setter
    def value_uom(self, value_uom):
        """
        Sets the value_uom of this DatafileOutputV1.
        The unit of measure to be used for every signal when importing signals and for every capsule property when importing a condition. If not specified, defaults to unitless. If a unit of measure row is specified, the information in that row overrides this setting.

        :param value_uom: The value_uom of this DatafileOutputV1.
        :type: str
        """

        self._value_uom = value_uom

    @property
    def value_uom_row(self):
        """
        Gets the value_uom_row of this DatafileOutputV1.
        Integer that identifies the row containing the unit of measure for each signal when importing signal(s) or for each capsule property when importing a condition. A setting of 0 indicates that there is no unit of measure header row. (Row 1 is the first row of the file.)

        :return: The value_uom_row of this DatafileOutputV1.
        :rtype: int
        """
        return self._value_uom_row

    @value_uom_row.setter
    def value_uom_row(self, value_uom_row):
        """
        Sets the value_uom_row of this DatafileOutputV1.
        Integer that identifies the row containing the unit of measure for each signal when importing signal(s) or for each capsule property when importing a condition. A setting of 0 indicates that there is no unit of measure header row. (Row 1 is the first row of the file.)

        :param value_uom_row: The value_uom_row of this DatafileOutputV1.
        :type: int
        """

        self._value_uom_row = value_uom_row

    def to_dict(self):
        """
        Returns the model properties as a dict
        """
        result = {}

        for attr, _ in iteritems(self.swagger_types):
            value = getattr(self, attr)
            if isinstance(value, list):
                result[attr] = list(map(
                    lambda x: x.to_dict() if hasattr(x, "to_dict") else x,
                    value
                ))
            elif hasattr(value, "to_dict"):
                result[attr] = value.to_dict()
            elif isinstance(value, dict):
                result[attr] = dict(map(
                    lambda item: (item[0], item[1].to_dict())
                    if hasattr(item[1], "to_dict") else item,
                    value.items()
                ))
            else:
                result[attr] = value

        return result

    def to_str(self):
        """
        Returns the string representation of the model
        """
        return pformat(self.to_dict())

    def __repr__(self):
        """
        For `print` and `pprint`
        """
        return self.to_str()

    def __eq__(self, other):
        """
        Returns true if both objects are equal
        """
        if not isinstance(other, DatafileOutputV1):
            return False

        return self.__dict__ == other.__dict__

    def __ne__(self, other):
        """
        Returns true if both objects are not equal
        """
        return not self == other
